{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5546e502-dbee-47fd-a094-c24233cd2f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 19:18:01.578158: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-09 19:18:01.639143: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-09 19:18:02.691800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0620fe24-8a4b-49ef-aa85-df2715155cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Toxicity                                              tweet\n",
      "0         0   @user when a father is dysfunctional and is s...\n"
     ]
    }
   ],
   "source": [
    "###Data Prep \n",
    "##1.curl -L -o ~/Workspace/dp/A.I./ML/DL/tf/Models/toxic-tweets-dataset.zip  https://www.kaggle.com/api/v1/datasets/download/ashwiniyer176/toxic-tweets-dataset\n",
    "##2.unzip toxic-tweets-dataset.zip && ls *.csv\n",
    "\n",
    "data_df = pd.read_csv(\"FinalBalancedDataset.csv\")\n",
    "data_df = data_df.drop(columns=[\"Unnamed: 0\"])\n",
    "print(data_df.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce8cff2-9e38-4450-8216-811da8b86657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxicity\n",
       "0    32592\n",
       "1    24153\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"Toxicity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46410e6b-168a-4d44-8b04-353c70986a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    shuffle_data = np.random.randint(len(data_df))\n",
    "    random_sample = data_df.iloc[shuffle_data]\n",
    "    #print(random_sample[\"Toxicity\"], random_sample[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fcce2e-c653-4613-b02c-99f7f941f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRE-PROCESSING\n",
    "#TextToVector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ee6c55-09f6-43e9-ae21-ac9d160e2c73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/miniconda3/envs/cancer/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56745,)\n",
      "(56745,)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tag = data_df[\"Toxicity\"]\n",
    "tag = np.array(tag)\n",
    "tweet = data_df[\"tweet\"]\n",
    "tweet = np.array(tweet)\n",
    "sentences = tweet.tolist()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenized_output = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(tag.shape)\n",
    "print(tweet.shape)\n",
    "#print(tokenized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f05de0-dfea-4763-8f67-97cf8e563625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((tokenized_output, tag))\n",
    "\n",
    "# Apply transformations\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(160000)\n",
    "dataset = dataset.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0872acc0-c6bb-4cc7-bbf8-3a1a2ae40042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "train = dataset.take(int(len(dataset)* .7))\n",
    "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.3))\n",
    "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c81190f7-8ccb-4d72-b4ee-502d38c11c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text-classifier\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 484)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 484, 32)              927872    ['input_ids[0][0]']           \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 64)                   16640     ['embedding_1[0][0]']         \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 128)                  8320      ['bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 256)                  33024     ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 128)                  32896     ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 484)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 1)                    129       ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1018881 (3.89 MB)\n",
      "Trainable params: 1018881 (3.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "<_TakeDataset element_spec=({'input_ids': TensorSpec(shape=(None, 484), dtype=tf.int64, name=None), 'token_type_ids': TensorSpec(shape=(None, 484), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 484), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
      "2482/2482 [==============================] - 1290s 518ms/step - loss: 0.2020 - accuracy: 0.9197 - val_loss: 0.1238 - val_accuracy: 0.9560\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense\n",
    "\n",
    "# Define input layers\n",
    "input_ids = Input(shape=(484,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(484,), dtype=tf.int32, name=\"attention_mask\")  # Optional\n",
    "\n",
    "# Embedding layer\n",
    "embedding = Embedding(input_dim=len(tokenizer.get_vocab()), output_dim=32)(input_ids)\n",
    "\n",
    "# Bidirectional LSTM layer\n",
    "bi_lstm = Bidirectional(LSTM(32, activation='tanh'))(embedding)\n",
    "\n",
    "# Fully connected layers\n",
    "fc_1 = Dense(128, activation='relu')(bi_lstm)\n",
    "fc_2 = Dense(256, activation='relu')(fc_1)\n",
    "fc_3 = Dense(128, activation='relu')(fc_2)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(fc_3)\n",
    "\n",
    "# Define model with multiple inputs\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output, name=\"text-classifier\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "print(train.take(1)) \n",
    "# Function to map datasets to the correct format for model input\n",
    "def map_dataset_to_input_and_labels(dataset):\n",
    "    return dataset.map(lambda x, y: ({\n",
    "        'input_ids': x['input_ids'],  # Access based on keys\n",
    "        'attention_mask': x['attention_mask']\n",
    "    }, y))\n",
    "\n",
    "\n",
    "# Map datasets to the correct format\n",
    "train_dataset = map_dataset_to_input_and_labels(train)\n",
    "val_dataset = map_dataset_to_input_and_labels(val)\n",
    "\n",
    "# Training the model with correctly formatted datasets\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1,\n",
    "    batch_size=16,\n",
    "    validation_data=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650b8cb-4b1a-4d81-acec-beb3607a02ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26049fc7-a000-4d42-b259-573ba8487a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
